{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msquareddd/ai-engineering-notebooks/blob/main/machine_time_prediction_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "39foGtXFsEPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "G6SjSGvr5lvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import optuna\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "UM3Ycwxaqacv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Reading"
      ],
      "metadata": {
        "id": "_JTJZ-65sHF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading excel file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename\n",
        "file_names = uploaded.keys()\n",
        "FILE_NAME = list(file_names)[0]\n",
        "# print(FILE_NAME)"
      ],
      "metadata": {
        "id": "s-slRf4ES7Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JlbeAugSPDS"
      },
      "outputs": [],
      "source": [
        "# Create dataframe\n",
        "dataset = pd.read_excel(FILE_NAME, sheet_name=\"Foglio3\")\n",
        "# dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "C3uJFgStsPxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns to more suitable names\n",
        "dataset.rename(columns={'NUM FACCE':\"NUM_FACCE\",\"RETT TANG\":\"RETT_TANG\",\n",
        "                        'EDM DRILL':\"EDM_DRILL\",\"EDM FILO\":\"EDM_FILO\",\n",
        "                        \"EDM TUFFO\":\"EDM_TUFFO\", \"RETT TANG.1\":\"RETT_TANG_1\",\n",
        "                        \"RETT TOND\":\"RETT_TOND\",\"RETT TOND.1\":\"RETT_TOND_1\",\n",
        "                        \"RETT PLAN\":\"RETT_PLAN\"},\n",
        "                 inplace=True)\n",
        "# print(dataset.columns)\n",
        "\n",
        "# Select feature columns\n",
        "feature_cols = ['CICLO', 'X', 'Y','Z','NUM_FACCE','MATERIALE','TRATTAMENTO',]\n",
        "\n",
        "# Select target columns\n",
        "target_cols = [col for col in dataset.columns if col not in feature_cols]\n",
        "\n",
        "# print(feature_cols)\n",
        "# print(target_cols)\n",
        "\n",
        "# Prepare empty dictionary to contain all models\n",
        "models_dict = {}"
      ],
      "metadata": {
        "id": "6iZdLf1Hi9XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy dataset to avoid spoiling original one\n",
        "features_df = dataset.copy()\n",
        "\n",
        "# Get cycle numbers\n",
        "cycle_list = features_df[\"CICLO\"].unique().tolist()\n",
        "# print(cycle_list)\n",
        "\n",
        "# Use vectorized operations to check for zeros\n",
        "piece_is_round_X = (features_df[\"X\"] == 0).any()\n",
        "piece_is_round_Y = (features_df[\"Y\"] == 0).any()\n",
        "\n",
        "# Add surface feature depending on shape\n",
        "if piece_is_round_X:\n",
        "    features_df[\"SUPERF\"] = (features_df[\"Y\"] / 2) ** 2 * np.pi\n",
        "elif piece_is_round_Y:\n",
        "    features_df[\"SUPERF\"] = (features_df[\"X\"] / 2) ** 2 * np.pi\n",
        "else:\n",
        "    features_df[\"SUPERF\"] = features_df[\"X\"] * features_df[\"Y\"]\n",
        "\n",
        "# Drop unnecessary columns in one go\n",
        "features_df.drop(target_cols + [\"X\", \"Y\"], axis=1, inplace=True)\n",
        "\n",
        "# print(features_df.head())\n"
      ],
      "metadata": {
        "id": "PcfOITB_StT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGB Hyperparameters Optimization"
      ],
      "metadata": {
        "id": "BsojC88QsYYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters optimization function\n",
        "def objective(trial):\n",
        "\n",
        "  params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 12000, step=100),\n",
        "        \"max_depth\":trial.suggest_int(\"max_depth\", 1, 5),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n",
        "        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0, step=0.1),\n",
        "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n",
        "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-6, 100.),\n",
        "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-6, 100.),\n",
        "    }\n",
        "\n",
        "\n",
        "  model = XGBRegressor(\n",
        "        **params,\n",
        "        n_jobs=-1,\n",
        "        # tree_method='gpu_hist',\n",
        "        gpu_id=0,\n",
        "        early_stopping_rounds=300\n",
        "        )\n",
        "\n",
        "  model.fit(X_train, y_train,\n",
        "              eval_set=[(X_test, y_test)],\n",
        "              verbose=0\n",
        "            )\n",
        "\n",
        "  y_hat = model.predict(X_test)\n",
        "\n",
        "  return mean_squared_error(y_test, y_hat)"
      ],
      "metadata": {
        "id": "1JJtRvyu5eUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGB Training and Fitting"
      ],
      "metadata": {
        "id": "DmYxkeit6-ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for cycle in cycle_list:\n",
        "  model_list = []\n",
        "  for col in target_cols:\n",
        "      y = dataset[dataset[\"CICLO\"] == cycle][col]\n",
        "\n",
        "      # Copy dataset to avoid corruption\n",
        "      features_df_copy = features_df.copy()\n",
        "\n",
        "      # Drop cycle column\n",
        "      features_df_copy.drop(\"CICLO\", axis=1, inplace=True)\n",
        "\n",
        "      # Identify categorical columns\n",
        "      cat_cols = features_df_copy.columns[features_df_copy.dtypes == \"object\"]\n",
        "\n",
        "      # Create a ColumnTransformer for one-hot encoding categorical variables\n",
        "      ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), cat_cols)],\n",
        "                            remainder='passthrough')\n",
        "\n",
        "      # Apply the transformation to all features\n",
        "      X_transformed = np.array(ct.fit_transform(features_df_copy))\n",
        "      # Get feature names after OHE to visualize importance later on\n",
        "      feature_names = ct.get_feature_names_out()\n",
        "\n",
        "      # print(X_transformed.shape)\n",
        "\n",
        "      # Use train_test_split for a hold-out evaluation\n",
        "      X_train, X_test, y_train, y_test = train_test_split(\n",
        "          X_transformed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "      # Hyperparameters optmization with optuna\n",
        "      study = optuna.create_study(direction=\"minimize\")\n",
        "      optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "      study.optimize(objective, n_trials=50)\n",
        "\n",
        "      best_params = study.best_params\n",
        "      best_mae = study.best_value\n",
        "      print(f\"Best MAE: {best_mae}\")\n",
        "      print(f\"Best params: {best_params}\")\n",
        "\n",
        "      # Define a pipeline with preprocessing and XGBRegressor\n",
        "      pipeline = Pipeline([\n",
        "          ('regressor', XGBRegressor(**best_params, random_state=42))\n",
        "      ])\n",
        "\n",
        "      fit_params = {\"regressor__eval_set\":[(X_test, y_test)],\n",
        "          \"regressor__verbose\":False\n",
        "      }\n",
        "\n",
        "      # Fit the pipeline\n",
        "      pipeline.fit(X_train, y_train, **fit_params)\n",
        "\n",
        "      # Predict test set\n",
        "      preds = pipeline.predict(X_test)\n",
        "\n",
        "      # Evaluation metrics\n",
        "      # Mean Absolute Error\n",
        "      result = mean_absolute_error(y_test, preds)\n",
        "      print(f\"{col} - Train/Test Split MAE: {result:.4f}\")\n",
        "\n",
        "      # Pipeline score\n",
        "      pip_score = pipeline.score(X_test, y_test)\n",
        "      print(f\"{col} - Train/Test Split R2 Score: {pip_score:.4f}\")\n",
        "\n",
        "      # Cross-validation\n",
        "      cv_scores = cross_val_score(pipeline, X_transformed, y, scoring='neg_mean_absolute_error', cv=5)\n",
        "      print(f\"{col} - CV MAE: {-np.mean(cv_scores):.4f}\")\n",
        "\n",
        "      # Visualize feature importance\n",
        "      importance = pipeline.steps[0][1].feature_importances_\n",
        "\n",
        "      sorted_idx = np.argsort(importance)[::-1]\n",
        "\n",
        "      plt.figure(figsize=(10, 6))\n",
        "      plt.barh([feature_names[i] for i in sorted_idx], importance[sorted_idx])\n",
        "      plt.xlabel(\"Feature Importance\")\n",
        "      plt.ylabel(\"Features\")\n",
        "      plt.title(\"XGBoost Feature Importance\")\n",
        "      plt.gca().invert_yaxis()\n",
        "      plt.show()\n",
        "\n",
        "      #Saving model\n",
        "      model_name = str(cycle) + \"_\" + col + '_model.pkl'\n",
        "      print(model_name)\n",
        "      model_list.append(model_name)\n",
        "      with open(model_name, 'wb') as file:\n",
        "          pickle.dump(pipeline, file)\n",
        "\n",
        "  # Updating dictionary with list of models for every cycle\n",
        "  models_dict[cycle] = model_list\n"
      ],
      "metadata": {
        "id": "zIRSw4AvS0YY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Single Prediction"
      ],
      "metadata": {
        "id": "mYQ7Jo_Ascxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# New list of values never seen by the models\n",
        "X_new = [5000,209,153,60,520,\"1.2379\",\"TR 1\"]\n",
        "\n",
        "# Convert X_new to a DataFrame\n",
        "X_new_df = pd.DataFrame([X_new], columns=feature_cols)\n",
        "\n",
        "# Use vectorized operations to check for zeros\n",
        "piece_is_round_X = (X_new_df[\"X\"] == 0).any()\n",
        "piece_is_round_Y = (X_new_df[\"Y\"] == 0).any()\n",
        "\n",
        "# Add surface feature depending on shape\n",
        "if piece_is_round_X:\n",
        "    X_new_df[\"SUPERF\"] = (X_new_df[\"Y\"] / 2) ** 2 * np.pi\n",
        "elif piece_is_round_Y:\n",
        "    X_new_df[\"SUPERF\"] = (X_new_df[\"X\"] / 2) ** 2 * np.pi\n",
        "else:\n",
        "    X_new_df[\"SUPERF\"] = X_new_df[\"X\"] * X_new_df[\"Y\"]\n",
        "\n",
        "# Drop unnecessary columns in one go\n",
        "X_new_df.drop([\"X\", \"Y\"], axis=1, inplace=True)\n",
        "\n",
        "# print(X_new_df.head())\n",
        "\n",
        "# Transform the new sample and encode categorical variables\n",
        "X_new_transformed = ct.transform(X_new_df)"
      ],
      "metadata": {
        "id": "2OYhCIz1uPTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the cycle number from new prediction values\n",
        "single_pred_cycle = X_new_df[\"CICLO\"].unique().tolist()[0]\n",
        "# print(single_pred_cycle)\n",
        "\n",
        "machine_times_list = []\n",
        "\n",
        "# Load every model and for each one predict machine time\n",
        "for model in models_dict[single_pred_cycle]:\n",
        "  # print(model)\n",
        "  with open(model, 'rb') as file:\n",
        "      current_model = pickle.load(file)\n",
        "      current_model_pred = current_model.predict(X_new_transformed)\n",
        "      # print(current_model_pred)\n",
        "      machine_times_list.append(current_model_pred[0])\n",
        "# print(machine_times_list)\n",
        "\n",
        "# Print machine times\n",
        "for i, time in enumerate(machine_times_list):\n",
        "  print(f\"{target_cols[i]}: {time:.1f}\")"
      ],
      "metadata": {
        "id": "xCfSsc5l1v1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}