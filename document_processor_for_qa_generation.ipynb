{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msquareddd/ai-engineering-notebooks/blob/main/document_processor_for_qa_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gSoSOGG-lwN"
      },
      "source": [
        "# Document Processor for Q&A Generation\n",
        "\n",
        "This notebook processes documents (DOCX, PPTX, PDF) and generates question-answer pairs using Hugging Face models.\n",
        "\n",
        "## How to use:\n",
        "1. Run the setup cell to install dependencies\n",
        "2. Configure your model settings\n",
        "3. Upload your document\n",
        "4. Generate Q&A pairs\n",
        "5. Download the results as JSON"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLzoC2b7-lwQ"
      },
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj-ove3T-lwQ"
      },
      "outputs": [],
      "source": [
        "# Check if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab\")\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Google Colab\")\n",
        "\n",
        "# Install required packages\n",
        "print(\"Installing required packages...\")\n",
        "!pip install -q docling transformers torch accelerate bitsandbytes triton kernels\n",
        "!pip install -q python-docx\n",
        "!pip install -q tqdm\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"Installing additional Colab-specific packages...\")\n",
        "    !pip install -q ipywidgets\n",
        "\n",
        "print(\"All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyS8cgH4-lwR"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import io\n",
        "import torch\n",
        "from datetime import date\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "import bitsandbytes\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Document processing\n",
        "from docling.document_converter import DocumentConverter\n",
        "\n",
        "# Hugging Face Transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "\n",
        "# Colab specific imports\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, JSON, clear_output\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5tGSzfP-lwS"
      },
      "source": [
        "## 2. Configuration and Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhz3keIx-lwS"
      },
      "outputs": [],
      "source": [
        "# System prompt for Q&A generation\n",
        "SYS_PROMPT = \"\"\"\n",
        "IMPORTANT: Your response must be raw JSON without any markdown formatting. Do NOT use ```json or ``` tags. Start your response directly with [ and end with ].\n",
        "IMPORTANT: Generate at least 15-20 Q&A pairs per document section.\n",
        "IMPORTANT: You **must** create specific Q&A pairs for any mention of key business entities if they are present in the document. This includes, but is not limited to: **project names, supplier names, client names, part numbers, product info, and other critical business-specific identifiers**\n",
        "IMPORTANT: Always reference the project name or client name in every Q&A pair.\n",
        "\n",
        "You are a specialized AI assistant expert in creating high-quality training datasets. Your purpose is to generate question-and-answer pairs from a given document to be used for fine-tuning a large language model.\n",
        "\n",
        "Your task is to read the provided document in Markdown format and generate a series of varied, high-quality, and natural-sounding question-and-answer pairs. Your entire response must be a single, valid JSON object and nothing else.\n",
        "\n",
        "### Your Guiding Principles\n",
        "\n",
        "1.  **Language Check and Translation**: Your first step is to identify the language of the provided document. If the document is not in English, you must translate the entire text into high-quality, fluent English before proceeding. All subsequent steps will be performed on this translated English version.\n",
        "2.  **Strictly Grounded**: All questions must be answerable from the provided text, and all answers must be derived exclusively from it. Do not use any external knowledge or make assumptions beyond what is written.\n",
        "3.  **High-Quality and Natural Language**: The Q&A pairs should be well-written, grammatically correct, and phrased in a way that a human would naturally ask and answer them.\n",
        "4.  **Comprehensive Coverage**: Aim to create questions that cover the main topics, key entities, processes, and conclusions mentioned in the document.\n",
        "5.  **Reference Components**: When discussing test results, alwyas include the names of the parts tested in Q&A pair.\n",
        "6.  **Reference Project Name/Client**: Always reference the project name or client name in every Q&A pair.\n",
        "\n",
        "### Instructions for Generating Questions\n",
        "\n",
        "-   **Prioritize Key Entities**: You **must** create specific Q&A pairs for any mention of key business entities if they are present in the document. This includes, but is not limited to: **project names, supplier names, client names, part numbers, product info, and other critical business-specific identifiers**. This information is highly valuable for the fine-tuning process.\n",
        "-   **Diverse Question Types**: Generate a mix of questions, including:\n",
        "    -   **Factual Recall**: Questions that ask for specific details, definitions, names, or numbers (e.g., \"What is the name of the framework?\", \"When was the company founded?\").\n",
        "    -   **Summarization**: Questions that require summarizing a paragraph or a concept (e.g., \"What are the main advantages of this approach?\", \"Can you summarize the findings of the study?\").\n",
        "    -   **Inferential**: Questions that require connecting information from different parts of the document (e.g., \"Based on the challenges mentioned, what was the likely reason for the project's delay?\").\n",
        "    -   **Procedural**: Questions that ask to explain a process or a sequence of steps (e.g., \"How does the system authenticate a user?\").\n",
        "-   **Varied Phrasing**: Avoid repetitive question structures. Use different ways to ask about the same or similar information to ensure a diverse dataset. For example, instead of only asking \"What is X?\", use variations like \"Can you explain what X is?\", \"Describe the purpose of X.\", or \"What role does X play?\".\n",
        "\n",
        "### Instructions for Generating Answers\n",
        "\n",
        "-   **Concise and Accurate**: Answers should be direct, to the point, and accurately reflect the information in the source text.\n",
        "-   **Natural Tone**: Write answers in a clear, conversational, and informative tone.\n",
        "-   **Self-Contained**: Each answer should make sense on its own without needing to read the entire source document.\n",
        "-   **No New Information**: Do not add any information, context, or explanations that are not explicitly present in the provided text.\n",
        "-   **Avoid Talking About Images**: If the documents mention particular points or results in charts or images do not include that part.\n",
        "\n",
        "### Constraints & Output Format\n",
        "\n",
        "-   **No Ambiguity**: If a section of the text is ambiguous or lacks sufficient detail to form a clear question and answer, skip it.\n",
        "-   **No Opinions**: Do not generate questions or answers that are subjective or require an opinion, unless the document itself presents an opinion and the question is about that stated opinion (e.g., \"What was the author's main criticism of the policy?\").\n",
        "-   **Strict JSON Output**: Your entire response **MUST** be only a single, valid JSON object. Do not include any introductory text (like \"Here is the JSON you requested:\"), explanations, or concluding remarks. **CRITICAL: Do NOT wrap your response in markdown code blocks (```json ... ```).** Your response must start directly with the opening bracket `[` and end with the closing bracket `]`. The root of the JSON object must be a list, where each element is an object containing two string keys: `\"question\"` and `\"answer\"`.\n",
        "\n",
        "### Enhanced Detail Instructions\n",
        "- **Multi-level Questions**: Generate questions at different depth levels:\n",
        "  - Surface-level: \"What is X?\"\n",
        "  - Detailed: \"Can you explain the key components and functions of X?\"\n",
        "  - Analytical: \"How does X compare to Y in terms of performance and cost?\"\n",
        "- **Follow-up Questions**: Create question chains where answers to one question lead to more detailed follow-ups\n",
        "- **Contextual Relationships**: Generate questions that explore relationships between different concepts in the document\n",
        "\n",
        "### Quantity Targets\n",
        "- **Minimum Output**: Generate at least 15-20 Q&A pairs per document section\n",
        "- **Depth Coverage**: For each major concept, create 3-4 questions exploring different aspects\n",
        "- **Cross-references**: Include questions that connect information from different sections\n",
        "\n",
        "### Example\n",
        "\n",
        "**If the input document is:**\n",
        "\"Project 'Phoenix' was initiated for our client, 'Innovate Corp', to overhaul their logistics. The main supplier for the new hardware is 'Tech Solutions Inc.'. The key component is the 'Sensor Model T-1000'. Two components were tested, component 1 and component 2. Stacks with smaller angular subdivision show reduced performance in polarization—and consequently in permeability—and in core loss values.\n",
        "\n",
        "**Your output MUST be ONLY the following JSON content:**\n",
        "[\n",
        "    {\n",
        "        \"question\": \"What is the name of the project initiated for Innovate Corp?\",\n",
        "        \"answer\": \"The project is named 'Phoenix'.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who is the client for the 'Phoenix' project?\",\n",
        "        \"answer\": \"The client for the 'Phoenix' project is 'Innovate Corp'.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"In the Phoenix project, which company is the main supplier for the new hardware?\",\n",
        "        \"answer\": \"In the Phoenix project, the main supplier for the new hardware is 'Tech Solutions Inc.'.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"In the Phoenix project, what is the name of the key component mentioned in the document?\",\n",
        "        \"answer\": \"In the Phoenix project, the key component is the 'Sensor Model T-1000'.\"\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"In the Phoenix project, what were the results of the magnetic tests performed on component 1 and component 2?\",\n",
        "        \"answer\": \"In the Phoenix project, the results of magnetic tests performed on component 1 and component 2 showed a 15% difference in permeability in favor of component 1\"\n",
        "    },\n",
        "]\n",
        "Now, carefully read the document provided below and generate the Q&A pairs according to these instructions.\n",
        "\"\"\"\n",
        "\n",
        "print(\"System prompt loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AbHvBWx-lwS"
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "class ModelConfig:\n",
        "    def __init__(self):\n",
        "        self.available_models = {\n",
        "            \"Mistral-7B Instruct\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "            \"Llama-3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "            \"Qwen2.5-14B-Instruct-1M\" : \"Qwen/Qwen2.5-14B-Instruct-1M\",\n",
        "            \"Qwen2.5-14B-Instruct\" : \"Qwen/Qwen2.5-14B-Instruct\",\n",
        "            \"Qwen2.5-14B\" : \"Qwen/Qwen2.5-14B\",\n",
        "            \"GPT-OSS-20B\": \"openai/gpt-oss-20b\",\n",
        "        }\n",
        "\n",
        "        # Default settings\n",
        "        self.model_name = \"Qwen2.5-14B-Instruct\"\n",
        "        self.model_id = self.available_models[self.model_name]\n",
        "        self.temperature = 0.7\n",
        "        self.max_new_tokens = 1024*3 # Reduced default value\n",
        "        self.top_p = 0.9\n",
        "        self.repetition_penalty = 1.1\n",
        "\n",
        "    def update_model(self, model_name):\n",
        "        if model_name in self.available_models:\n",
        "            self.model_name = model_name\n",
        "            self.model_id = self.available_models[model_name]\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Create model configuration instance\n",
        "model_config = ModelConfig()\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Create model selection dropdown\n",
        "    model_dropdown = widgets.Dropdown(\n",
        "        options=list(model_config.available_models.keys()),\n",
        "        value=model_config.model_name,\n",
        "        description='Select Model:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    # Create parameter sliders\n",
        "    temp_slider = widgets.FloatSlider(\n",
        "        value=model_config.temperature,\n",
        "        min=0.1,\n",
        "        max=2.0,\n",
        "        step=0.1,\n",
        "        description='Temperature:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    max_tokens_slider = widgets.IntSlider(\n",
        "        value=model_config.max_new_tokens,\n",
        "        min=512,\n",
        "        max=8192,\n",
        "        step=512,\n",
        "        description='Max Tokens:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    # Display configuration widgets\n",
        "    print(\"Configure your model settings:\")\n",
        "    display(model_dropdown, temp_slider, max_tokens_slider)\n",
        "else:\n",
        "    print(f\"Default model: {model_config.model_name}\")\n",
        "    print(f\"Temperature: {model_config.temperature}\")\n",
        "    print(f\"Max tokens: {model_config.max_new_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXrzN2eo-lwT"
      },
      "source": [
        "## 3. Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-DDojLS-lwT"
      },
      "outputs": [],
      "source": [
        "# Update model configuration based on user selection\n",
        "if IN_COLAB:\n",
        "    model_config.update_model(model_dropdown.value)\n",
        "    model_config.temperature = temp_slider.value\n",
        "    model_config.max_new_tokens = max_tokens_slider.value\n",
        "\n",
        "print(f\"Loading model: {model_config.model_name} ({model_config.model_id})\")\n",
        "print(f\"This may take a few minutes...\")\n",
        "\n",
        "# Check for GPU availability\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_config.model_id)\n",
        "\n",
        "    # Prepare keyword arguments for model loading\n",
        "    model_kwargs = {\n",
        "        \"dtype\": torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        \"device_map\": \"auto\" if device == \"cuda\" else None,\n",
        "    }\n",
        "\n",
        "    # Add quantization config only if not using GPT-OSS-20B\n",
        "    if model_config.model_name != \"GPT-OSS-20B\":\n",
        "        model_kwargs[\"quantization_config\"] = quant_config\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_config.model_id,\n",
        "        **model_kwargs\n",
        "    )\n",
        "\n",
        "    # Create text generation pipeline\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=model_config.max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    print(f\"Model loaded successfully on {device}!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {str(e)}\")\n",
        "    print(\"Please try a different model or check your internet connection.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRAOqWJ7-lwT"
      },
      "source": [
        "## 4. Document Upload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CPzT03p-lwU"
      },
      "outputs": [],
      "source": [
        "# Initialize document converter\n",
        "converter = DocumentConverter()\n",
        "\n",
        "# Function to upload and process document\n",
        "def upload_document():\n",
        "    if IN_COLAB:\n",
        "        print(\"Please upload your document (DOCX, PPTX, PDF):\")\n",
        "        uploaded = files.upload()\n",
        "\n",
        "        if not uploaded:\n",
        "            raise ValueError(\"No file uploaded. Please select a file to continue.\")\n",
        "\n",
        "        # Get the uploaded file path\n",
        "        file_path = list(uploaded.keys())[0]\n",
        "        print(f\"File uploaded: {file_path}\")\n",
        "\n",
        "        return file_path\n",
        "    else:\n",
        "        # For local execution, use a default path or ask for input\n",
        "        file_path = input(\"Enter the path to your document: \")\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "        return file_path\n",
        "\n",
        "# Function to convert document to markdown\n",
        "def convert_to_markdown(file_path):\n",
        "    print(f\"Converting {file_path} to markdown...\")\n",
        "\n",
        "    try:\n",
        "        result = converter.convert(file_path)\n",
        "        markdown_content = result.document.export_to_markdown()\n",
        "        print(\"Document converted successfully!\")\n",
        "        return markdown_content\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting document: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Upload and convert document\n",
        "try:\n",
        "    uploaded_file = upload_document()\n",
        "    doc_markdown = convert_to_markdown(uploaded_file)\n",
        "\n",
        "    # Display a preview of the converted content\n",
        "    print(\"\\nDocument preview (first 500 characters):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(doc_markdown[:500] + \"...\" if len(doc_markdown) > 500 else doc_markdown)\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"Total document length: {len(doc_markdown)} characters\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    print(\"Please try uploading a different document.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwpdDcXO-lwU"
      },
      "source": [
        "## 5. Q&A Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wPW8Qh8-lwU"
      },
      "outputs": [],
      "source": [
        "# Utility functions for JSON parsing\n",
        "def clean_json_response(response_text):\n",
        "    \"\"\"\n",
        "    Clean JSON response that might be wrapped in markdown tags.\n",
        "    \"\"\"\n",
        "    # Remove markdown code blocks if present\n",
        "    json_pattern = r'```(?:json)?\\s*(.*?)\\s*```'\n",
        "    match = re.search(json_pattern, response_text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "    if match:\n",
        "        # Extract JSON from within markdown tags\n",
        "        cleaned_json = match.group(1).strip()\n",
        "        return cleaned_json\n",
        "    else:\n",
        "        # If no markdown tags found, return the original text\n",
        "        return response_text.strip()\n",
        "\n",
        "def parse_llm_response(response_text):\n",
        "    \"\"\"\n",
        "    Parse LLM response, handling both raw JSON and markdown-wrapped JSON.\n",
        "    \"\"\"\n",
        "    # First try to parse the response as-is\n",
        "    try:\n",
        "        return json.loads(response_text)\n",
        "    except json.JSONDecodeError:\n",
        "        # If that fails, try cleaning markdown tags and parsing again\n",
        "        cleaned_response = clean_json_response(response_text)\n",
        "        return json.loads(cleaned_response)\n",
        "\n",
        "# Function to generate Q&A pairs\n",
        "def generate_qa_pairs(markdown_content):\n",
        "    print(\"Generating Q&A pairs...\")\n",
        "    print(\"This may take a few minutes depending on the document size.\")\n",
        "\n",
        "    # Prepare the prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
        "        {\"role\": \"user\", \"content\": markdown_content}\n",
        "    ]\n",
        "\n",
        "    # Format messages for the model\n",
        "    if \"Instruct\" in model_config.model_name:\n",
        "        # For models that use chat templates\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "        print(\"Chat template applied successfully!\")\n",
        "    else:\n",
        "        # For other models, combine messages manually\n",
        "        prompt = f\"{SYS_PROMPT}\\n\\nDocument:\\n{markdown_content}\\n\\nGenerate Q&A pairs:\"\n",
        "\n",
        "    try:\n",
        "        # Generate response\n",
        "        with tqdm(total=100, desc=\"Generating Q&A pairs\") as pbar:\n",
        "            response = generator(\n",
        "                prompt,\n",
        "                max_new_tokens=model_config.max_new_tokens,\n",
        "                temperature=model_config.temperature,\n",
        "                top_p=model_config.top_p,\n",
        "                repetition_penalty=model_config.repetition_penalty,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                return_full_text=False\n",
        "            )\n",
        "            pbar.update(100)\n",
        "\n",
        "        # Extract the generated text\n",
        "        generated_text = response[0]['generated_text']\n",
        "\n",
        "        print(\"\\nQ&A generation completed!\")\n",
        "        return generated_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating Q&A pairs: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Generate Q&A pairs\n",
        "try:\n",
        "    if 'doc_markdown' in locals():\n",
        "        llm_response = generate_qa_pairs(doc_markdown)\n",
        "\n",
        "        # Parse the response\n",
        "        print(\"\\nParsing response...\")\n",
        "        qa_data = parse_llm_response(llm_response)\n",
        "\n",
        "        print(f\"Successfully generated {len(qa_data)} Q&A pairs!\")\n",
        "\n",
        "        # Display first few Q&A pairs as preview\n",
        "        print(\"\\nPreview of generated Q&A pairs:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, qa in enumerate(qa_data[:3]):\n",
        "            print(f\"Q{i+1}: {qa['question']}\")\n",
        "            print(f\"A{i+1}: {qa['answer']}\")\n",
        "            print()\n",
        "        print(\"-\" * 50)\n",
        "        print(f\"Showing 3 of {len(qa_data)} Q&A pairs\")\n",
        "    else:\n",
        "        print(\"No document processed. Please upload a document first.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error: {str(e)}\")\n",
        "    print(\"Please try again with a different document or model.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biBIcRX3-lwU"
      },
      "source": [
        "## 6. Results and Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzwSEVSE-lwU"
      },
      "outputs": [],
      "source": [
        "# Function to save and download results\n",
        "def save_and_download_results(qa_data):\n",
        "    # Generate filename with current date\n",
        "    today = date.today()\n",
        "    filename = f\"{today.strftime('%Y%m%d')}_qa_dataset.json\"\n",
        "\n",
        "    # Save to file\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(qa_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"Results saved to {filename}\")\n",
        "\n",
        "    # Download file in Colab\n",
        "    if IN_COLAB:\n",
        "        files.download(filename)\n",
        "        print(\"Download initiated!\")\n",
        "    else:\n",
        "        print(f\"File saved locally at: {os.path.abspath(filename)}\")\n",
        "\n",
        "    return filename\n",
        "\n",
        "# Save and download results if available\n",
        "try:\n",
        "    if 'qa_data' in locals():\n",
        "        output_file = save_and_download_results(qa_data)\n",
        "\n",
        "        # Display statistics\n",
        "        print(\"\\nGeneration Statistics:\")\n",
        "        print(f\"- Total Q&A pairs: {len(qa_data)}\")\n",
        "        print(f\"- Average question length: {sum(len(qa['question']) for qa in qa_data) / len(qa_data):.1f} characters\")\n",
        "        print(f\"- Average answer length: {sum(len(qa['answer']) for qa in qa_data) / len(qa_data):.1f} characters\")\n",
        "        print(f\"- Model used: {model_config.model_name}\")\n",
        "        print(f\"- Document processed: {uploaded_file}\")\n",
        "\n",
        "        # Display full results in Colab\n",
        "        if IN_COLAB:\n",
        "            print(\"\\nFull Results:\")\n",
        "            display(JSON(qa_data))\n",
        "    else:\n",
        "        print(\"No Q&A data to save. Please generate Q&A pairs first.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saving results: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H38w_GxX-lwV"
      },
      "source": [
        "## 7. Process Multiple Documents (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2PDNhsD-lwV"
      },
      "outputs": [],
      "source": [
        "# Optional: Batch processing for multiple documents\n",
        "def process_multiple_documents():\n",
        "    if not IN_COLAB:\n",
        "        print(\"Batch processing is optimized for Google Colab.\")\n",
        "        return\n",
        "\n",
        "    print(\"Upload multiple documents for batch processing:\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"No files uploaded.\")\n",
        "        return\n",
        "\n",
        "    all_qa_data = []\n",
        "\n",
        "    for filename in tqdm(uploaded.keys(), desc=\"Processing documents\"):\n",
        "        try:\n",
        "            print(f\"\\nProcessing {filename}...\")\n",
        "\n",
        "            # Convert to markdown\n",
        "            doc_markdown = convert_to_markdown(filename)\n",
        "\n",
        "            # Generate Q&A pairs\n",
        "            llm_response = generate_qa_pairs(doc_markdown)\n",
        "            qa_data = parse_llm_response(llm_response)\n",
        "\n",
        "            # Add source document info\n",
        "            for qa in qa_data:\n",
        "                qa['source_document'] = filename\n",
        "\n",
        "            all_qa_data.extend(qa_data)\n",
        "            print(f\"Generated {len(qa_data)} Q&A pairs from {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    if all_qa_data:\n",
        "        # Save combined results\n",
        "        today = date.today()\n",
        "        filename = f\"{today.strftime('%Y%m%d')}_batch_qa_dataset.json\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(all_qa_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "        print(f\"\\nBatch processing completed!\")\n",
        "        print(f\"Total Q&A pairs generated: {len(all_qa_data)}\")\n",
        "        print(f\"Results saved to: {filename}\")\n",
        "\n",
        "        files.download(filename)\n",
        "    else:\n",
        "        print(\"No Q&A pairs were generated.\")\n",
        "\n",
        "# Uncomment the line below to run batch processing\n",
        "process_multiple_documents()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsd5EuYn-lwV"
      },
      "source": [
        "## Usage Instructions\n",
        "\n",
        "### Single Document Processing:\n",
        "1. Run all cells in order\n",
        "2. Configure your model settings in Section 2\n",
        "3. Upload your document when prompted in Section 4\n",
        "4. Wait for Q&A generation to complete\n",
        "5. Download the JSON file with the results\n",
        "\n",
        "### Batch Processing (Optional):\n",
        "1. Uncomment the last line in Section 7\n",
        "2. Run the cell to upload multiple documents\n",
        "3. Wait for all documents to be processed\n",
        "4. Download the combined results\n",
        "\n",
        "### Tips:\n",
        "- For large documents, consider using a smaller model or reducing max tokens\n",
        "- If you encounter memory errors, try restarting the runtime and using a smaller model\n",
        "- The quality of Q&A pairs depends on the model used and the document content\n",
        "- Some models may require authentication tokens from Hugging Face\n",
        "\n",
        "### Troubleshooting:\n",
        "- **Memory Issues**: Try using a smaller model or enable 8-bit quantization\n",
        "- **Slow Processing**: Consider using GPU runtime in Colab\n",
        "- **JSON Parsing Errors**: The model may not have followed the output format exactly - try regenerating\n",
        "- **Document Conversion Errors**: Ensure your document is in a supported format (DOCX, PPTX, PDF)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}