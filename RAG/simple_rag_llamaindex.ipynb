{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msquareddd/ai-engineering-notebooks/blob/main/simple_rag_llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Installations"
      ],
      "metadata": {
        "id": "KMZ3mHmgaBFa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OC1rDKDQPNyP"
      },
      "outputs": [],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate\n",
        "!pip install -q llama-index-llms-ollama llama-index-embeddings-huggingface\n",
        "!pip install -q llama-index-llms-huggingface llama-index-llms-langchain\n",
        "!pip install -q llama-index-readers-file\n",
        "!pip install -q docx2txt pypdf\n",
        "!pip install -q langchain langchain_community langchain_core langgraph huggingface_hub\n",
        "!pip install -U -q bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "9u3havnqaGAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "import gradio as gr\n",
        "from llama_index.llms.langchain import LangChainLLM\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "from llama_index.core.retrievers import VectorIndexRetriever\n",
        "from llama_index.core.query_engine import RetrieverQueryEngine\n",
        "from llama_index.core.postprocessor import (\n",
        "    SimilarityPostprocessor,\n",
        "    KeywordNodePostprocessor,\n",
        "    MetadataReplacementPostProcessor,\n",
        "    LongContextReorder,\n",
        ")\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig,Gemma3ForConditionalGeneration,\n",
        "                          AutoProcessor)\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_core.callbacks import StdOutCallbackHandler\n",
        "from transformers import pipeline\n",
        "from google.colab import userdata, files\n",
        "from huggingface_hub import login\n",
        "from langchain.schema import BaseRetriever, Document\n",
        "from typing import Any, Optional, List\n",
        "from pydantic import Field, ConfigDict"
      ],
      "metadata": {
        "id": "eApEkdg7Smko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HF Log-in"
      ],
      "metadata": {
        "id": "dd59OcqRaI2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os, getpass\n",
        "\n",
        "# def _set_env(var: str):\n",
        "#     if not os.environ.get(var):\n",
        "#         os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "# _set_env(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Set up LangSmith (optional)\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"RAG LlamaIndex\"\n",
        "\n",
        "# HuggingFace login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "0mNF7fSoQWBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Directory Creation"
      ],
      "metadata": {
        "id": "v6mvCJSYaOG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/data\"\n",
        "\n",
        "if not os.path.exists(base_path):\n",
        "    os.makedirs(base_path)\n",
        "    print(f\"Directory '{base_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{base_path}' already exists.\")"
      ],
      "metadata": {
        "id": "2eELODGhfwws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Initialization"
      ],
      "metadata": {
        "id": "v33K3XngaQju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_MODEL = \"google/gemma-3-4b-it\"#\"meta-llama/Llama-3.2-3B-Instruct\" #\"HuggingFaceTB/SmolLM3-3B\" ## \"meta-llama/Llama-3.1-8B-Instruct\" #\"Qwen/Qwen3-14B\" \"MiniMaxAI/MiniMax-M1-80k\"\n",
        "EMBED_MODEL = \"google/embeddinggemma-300m\"#\"BAAI/bge-m3\" #  \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\" \"nomic-ai/nomic-embed-text-v1.5\" \"BAAI/bge-base-en-v1.5\""
      ],
      "metadata": {
        "id": "_GbIPj9NUPWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization config\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_8bit_quant_type=\"int8\",\n",
        "    bnb_8bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# Load tokenizer with proper configuration\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
        "# processor = AutoProcessor.from_pretrained(LLM_MODEL)\n",
        "\n",
        "# Configure tokenizer for Gemma\n",
        "# if tokenizer.pad_token is None:\n",
        "#     tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.padding_side = \"left\"\n",
        "# tokenizer.model_max_length = 2048\n",
        "\n",
        "# Load model\n",
        "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
        "    LLM_MODEL,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    torch_dtype=torch.bfloat16,  # Use bfloat16\n",
        "    # attn_implementation=\"sdpa\",  # Scaled dot-product attention\n",
        "    # low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# Create pipeline\n",
        "# llm_pipeline = pipeline(\n",
        "#     \"image-text-to-text\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     return_full_text=False,\n",
        "#     max_new_tokens=256,\n",
        "#     do_sample=True,\n",
        "#     temperature=0.3,\n",
        "#     top_p=0.9,\n",
        "#     repetition_penalty=1.1,\n",
        "#     # pad_token_id=tokenizer.pad_token_id,\n",
        "#     # eos_token_id=tokenizer.eos_token_id,\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "#     truncation=True,\n",
        "#     max_length=2048,\n",
        "# )\n",
        "\n",
        "llm_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.3,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1,\n",
        "    # device=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Wrap with LangChain\n",
        "llm = HuggingFacePipeline(pipeline=llm_pipeline)\n",
        "print(\"âœ… Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "illAHLsUUQFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "m6BUyIil8gcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LlamaIndex Settings"
      ],
      "metadata": {
        "id": "4rop-FgvaXIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)\n",
        "Settings.llm = LangChainLLM(llm)  # llm is your HuggingFacePipeline instance\n",
        "# Settings.tokenizer = tokenizer\n",
        "# Settings.llm = HuggingFaceLLM(model_name=LLM_MODEL)\n",
        "# Settings.chunk_size = 512\n",
        "# Settings.chunk_overlap = 20"
      ],
      "metadata": {
        "id": "H0JIgSztSobf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a node parser\n",
        "node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)"
      ],
      "metadata": {
        "id": "_HxmwtdUr7u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "\n",
        "documents = SimpleDirectoryReader(\"/content/data\").load_data()\n",
        "\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Document {i+1}: {doc.metadata.get('file_name', 'Unknown')} - {len(doc.text)} characters\")\n",
        "\n",
        "nodes = node_parser.get_nodes_from_documents(documents, show_progress=True)\n",
        "print(f\"Created {len(nodes)} text chunks\")\n",
        "\n",
        "# query_engine = index.as_query_engine()"
      ],
      "metadata": {
        "id": "as4Aq9GSPZgK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex(nodes, show_progress=True)\n",
        "print(\"Index created successfully!\")"
      ],
      "metadata": {
        "id": "JAECf6Eksb29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 3\n",
        "\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index,\n",
        "    similarity_top_k=top_k,\n",
        ")"
      ],
      "metadata": {
        "id": "_Mxf8uUamEkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    node_postprocessors=[\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.2),\n",
        "        # KeywordNodePostprocessor(keywords=[\"important\", \"critical\"]),  # Filter by keywords\n",
        "        LongContextReorder(),  # Reorder nodes for better context window usage\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "2_anmxykmHQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Wrapper"
      ],
      "metadata": {
        "id": "RjaWZztqab5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LlamaIndexRetrieverWrapper(BaseRetriever):\n",
        "    # Define fields explicitly for Pydantic\n",
        "    query_engine: Optional[Any] = Field(default=None, exclude=True)\n",
        "    llamaindex_retriever: Optional[Any] = Field(default=None, exclude=True)\n",
        "    use_engine: bool = Field(default=False, exclude=True)\n",
        "    top_k: int = Field(default=3, exclude=True)\n",
        "\n",
        "    def __init__(self, query_engine=None, llamaindex_retriever=None, top_k=3, **kwargs):\n",
        "        # Initialize parent with any additional kwargs\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Option 1: Use a complete query engine (preferred)\n",
        "        if query_engine:\n",
        "            object.__setattr__(self, 'query_engine', query_engine)\n",
        "            object.__setattr__(self, 'use_engine', True)\n",
        "        # Option 2: Use just the retriever\n",
        "        elif llamaindex_retriever:\n",
        "            object.__setattr__(self, 'llamaindex_retriever', llamaindex_retriever)\n",
        "            object.__setattr__(self, 'use_engine', False)\n",
        "        else:\n",
        "            raise ValueError(\"Must provide either query_engine or llamaindex_retriever\")\n",
        "\n",
        "        object.__setattr__(self, 'top_k', top_k)\n",
        "\n",
        "    def _get_relevant_documents(self, query: str) -> List[Document]:\n",
        "        docs = []\n",
        "\n",
        "        if self.use_engine:\n",
        "            # Use the query engine which includes postprocessing\n",
        "            response = self.query_engine.query(query)\n",
        "\n",
        "            # Get nodes from the response (already postprocessed)\n",
        "            for node in response.source_nodes[:self.top_k]:\n",
        "                # The SimilarityPostprocessor has already filtered these\n",
        "                # Build metadata dictionary safely\n",
        "                metadata = {}\n",
        "                if hasattr(node, 'metadata') and node.metadata:\n",
        "                    metadata.update(node.metadata)\n",
        "                if hasattr(node, 'score'):\n",
        "                    metadata['score'] = node.score\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=node.text,\n",
        "                    metadata=metadata\n",
        "                )\n",
        "                docs.append(doc)\n",
        "\n",
        "                print(f\"with enginge - doc appended: {doc}\")\n",
        "        else:\n",
        "            # Fallback to direct retriever usage\n",
        "            nodes = self.llamaindex_retriever.retrieve(query)\n",
        "\n",
        "            for node in nodes[:self.top_k]:\n",
        "                # Manual similarity cutoff since no postprocessor\n",
        "                if hasattr(node, 'score') and node.score < 0.5:\n",
        "                    continue\n",
        "\n",
        "                # Build metadata dictionary safely\n",
        "                metadata = {}\n",
        "                if hasattr(node, 'metadata') and node.metadata:\n",
        "                    metadata.update(node.metadata)\n",
        "                if hasattr(node, 'score'):\n",
        "                    metadata['score'] = node.score\n",
        "\n",
        "                doc = Document(\n",
        "                    page_content=node.text,\n",
        "                    metadata=metadata\n",
        "                )\n",
        "                docs.append(doc)\n",
        "\n",
        "                print(f\"no enginge - doc appended: {doc}\")\n",
        "\n",
        "        return docs\n",
        "\n",
        "    async def _aget_relevant_documents(self, query: str) -> List[Document]:\n",
        "        # For async support (optional)\n",
        "        return self._get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "U_MAwxOtcbuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the LangChain-compatible retriever\n",
        "langchain_retriever = LlamaIndexRetrieverWrapper(query_engine=query_engine, llamaindex_retriever=retriever, top_k=top_k)"
      ],
      "metadata": {
        "id": "SLmdY4y6c7PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Template"
      ],
      "metadata": {
        "id": "IP7zXZCqaf6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template\n",
        "prompt_template = \"\"\"You are a helpful multilingual assistant answering questions based on provided documents and context.\n",
        "The documents may be in English or Italian. You should answer in the same language as the question.\n",
        "\n",
        "Instructions:\n",
        "- Answer based ONLY on the provided context\n",
        "- Be concise and direct\n",
        "- If the answer is not in the context, say \"I don't have enough information\" (or \"Non ho abbastanza informazioni\" in Italian)\n",
        "- Respond in the same language as the question\n",
        "- The documents provided might not be related to the topic, feel free to ignore them if you deem them unrelated\n",
        "- Just provide a direct answer to the question asked\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template,\n",
        "    input_variables=[\"context\", \"question\"]\n",
        ")"
      ],
      "metadata": {
        "id": "DYdgiFMbnDcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"question\"\n",
        "\n",
        "# response = query_engine.query(question)\n",
        "# # print(response)\n",
        "\n",
        "# context = \"Context:\\n\"\n",
        "# # Iterate only up to the number of source nodes returned\n",
        "# for i in range(top_k):\n",
        "#     context = context + response.source_nodes[i].text + \"\\n\\n\"\n",
        "\n",
        "# # print(context)\n",
        "\n",
        "# input=PROMPT.format(\n",
        "#             context=context,\n",
        "#             question=question)\n",
        "\n",
        "# # print(input)"
      ],
      "metadata": {
        "id": "7G32j2hcuhF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain Set-up"
      ],
      "metadata": {
        "id": "7CJm71Qpaj0X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up conversation memory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key='chat_history',\n",
        "    return_messages=True,\n",
        "    output_key='answer'\n",
        ")"
      ],
      "metadata": {
        "id": "viPDU4U7dBQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the conversational chain with the wrapped retriever\n",
        "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=langchain_retriever,  # Use the wrapped retriever\n",
        "    memory=memory,\n",
        "    callbacks=[StdOutCallbackHandler()],\n",
        "    combine_docs_chain_kwargs={\"prompt\": PROMPT},\n",
        "    return_source_documents=True,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "DOll6TnVZeMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cSZ-uT0D_SAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query"
      ],
      "metadata": {
        "id": "cMGQs9BXaqC-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = \"how was this problem solved?\""
      ],
      "metadata": {
        "id": "MroRLdFX5Abm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# docs = langchain_retriever.get_relevant_documents(message)\n",
        "# print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "Q-Ve4xoc9Xdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = conversation_chain.invoke({\"question\": message})\n",
        "# print(result)\n",
        "answer = result.get(\"answer\", \"Sorry, I couldn't process your question.\")\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "1e_TTt7K48gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Chat Interface"
      ],
      "metadata": {
        "id": "GuOqL0GGatjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Gradio chat interface\n",
        "def chat_function(message, history):\n",
        "    try:\n",
        "        # Get response from the conversation chain\n",
        "        result = conversation_chain.invoke({\"question\": message})\n",
        "\n",
        "        # Extract answer\n",
        "        answer = result.get(\"answer\", \"Sorry, I couldn't process your question.\")\n",
        "\n",
        "        # Optionally, you can also access source documents\n",
        "        sources = result.get(\"source_documents\", [])\n",
        "        if sources:\n",
        "            answer += \"\\n\\nðŸ“š Sources used:\"\n",
        "            for i, doc in enumerate(sources[:3], 1):\n",
        "                snippet = doc.page_content[:100] + \"...\" if len(doc.page_content) > 100 else doc.page_content\n",
        "                answer += f\"\\n{i}. {snippet}\"\n",
        "\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\""
      ],
      "metadata": {
        "id": "C0GgFeYYZihb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_interface = gr.ChatInterface(\n",
        "    fn=chat_function,\n",
        "    title=\"RAG LlamaIndex\",\n",
        "    description=\"Ask questions about your documents. The bot remembers conversation history!\",\n",
        "    examples=[\n",
        "        \"Can you summarize the activities performed in the [name] project?\",\n",
        "        \"What are the main findings?\",\n",
        "        \"Tell me more about the first point you mentioned\"\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "00fYAjJLZk4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "Af0fTWbUEHyU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
