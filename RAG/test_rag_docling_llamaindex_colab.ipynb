{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msquareddd/ai-engineering-notebooks/blob/main/RAG/test_rag_docling_llamaindex_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQmkP9Hz5qk9"
      },
      "source": [
        "# RAG with Docling/LlamaIndex/LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "eT_vFnH0GZ9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q \"pydantic>=2.12.0\"\n",
        "%pip install -q \\\n",
        "  llama-index-core \\\n",
        "  llama-index-llms-vllm \\\n",
        "  llama-index-readers-docling \\\n",
        "  llama-index-readers-file \\\n",
        "  llama-index-node-parser-docling \\\n",
        "  llama-index-embeddings-huggingface \\\n",
        "  llama-index-vector-stores-faiss \\\n",
        "  llama-index-embeddings-vllm \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  langchain-openai \\\n",
        "  llama-index-llms-langchain \\\n",
        "  vllm \\\n",
        "  transformers \\\n",
        "  bitsandbytes \\\n",
        "  accelerate \\\n",
        "  faiss-cpu \\\n",
        "  huggingface_hub \\\n",
        "  python-dotenv"
      ],
      "metadata": {
        "id": "2kDONwfYLpit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnu3rWcw5qlA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "from dotenv import load_dotenv\n",
        "import faiss\n",
        "\n",
        "filterwarnings(action=\"ignore\", category=UserWarning, module=\"pydantic\")\n",
        "filterwarnings(action=\"ignore\", category=FutureWarning, module=\"easyocr\")\n",
        "\n",
        "# Workaround for HuggingFace Tokenizers parallelism issues\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "print(\"Dependencies loaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Login"
      ],
      "metadata": {
        "id": "iq7fA5t1HGoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Set up LangSmith (optional)\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"Test RAG Docling/LlamaIndex\"\n",
        "\n",
        "# HuggingFace login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "9kMf3D7eeI_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Folder Creation"
      ],
      "metadata": {
        "id": "rm89_v9hhCLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/data\"\n",
        "\n",
        "if not os.path.exists(base_path):\n",
        "    os.makedirs(base_path)\n",
        "    print(f\"Directory '{base_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{base_path}' already exists.\")"
      ],
      "metadata": {
        "id": "cTlaJMxhhBKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJGJ8tS5qlA"
      },
      "source": [
        "# Main Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIfeJNaC3Qi3"
      },
      "outputs": [],
      "source": [
        "MODEL = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"#\"mistralai/Magistral-Small-2509\"\n",
        "VLLM_MODEL = \"nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16\" #\"Qwen/Qwen2.5-14B-Instruct-AWQ\" #\"cpatonn/NVIDIA-Nemotron-Nano-12B-v2-AWQ-8bit\" # # \"Qwen/Qwen2.5-14B-Thinking-AWQ\" \"Qwen/Qwen2.5-14B-Instruct-AWQ\" #\"Qwen/Qwen2.5-14B-Instruct\"\n",
        "EMBED_MODEL = \"google/embeddinggemma-300m\" #\"BAAI/bge-small-en-v1.5\"\n",
        "DB_PATH = \"./data/faiss_index.bin\"\n",
        "MAX_NEW_TOKENS = 1024\n",
        "MAX_MODEL_LEN = 4096*2\n",
        "QUANT = None #\"awq_marlin\" #\"compressed-tensors\" #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uuRHEVXp-xH"
      },
      "source": [
        "# Using Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEb_2Drp-xH"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# import torch\n",
        "# from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# quant_config_4bit = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "\n",
        "# llm = HuggingFaceLLM(\n",
        "#     model_name=MODEL,\n",
        "#     tokenizer_name=MODEL,\n",
        "#     context_window=8000, # Increased context window size\n",
        "#     max_new_tokens=MAX_NEW_TOKENS,\n",
        "#     model_kwargs={\"quantization_config\": quant_config_4bit, \"dtype\": torch.bfloat16},\n",
        "#     generate_kwargs={\"temperature\": 0.7, \"do_sample\": True,\"top_p\": 0.95},\n",
        "#     device_map=\"auto\",\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using vLLM"
      ],
      "metadata": {
        "id": "qQDbGK-k3q3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM LangChain Wrapper"
      ],
      "metadata": {
        "id": "I0MuwfGIXdtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.llms import VLLM\n",
        "\n",
        "# llm = VLLM(model=VLLM_MODEL,\n",
        "#            trust_remote_code=True,\n",
        "#            max_new_tokens=MAX_NEW_TOKENS,\n",
        "#            top_k=10,\n",
        "#            top_p=0.95,\n",
        "#            temperature=0.8,\n",
        "#            tensor_parallel_size=1,\n",
        "#            vllm_kwargs={\n",
        "#             \"quantization\": QUANT,\n",
        "#             \"swap_space\": 1,\n",
        "#             \"gpu_memory_utilization\": 0.8,\n",
        "#             \"max_model_len\": 4096,\n",
        "#            }\n",
        "# )\n"
      ],
      "metadata": {
        "id": "Yt7f2k0Khe_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## vLLM Server for LangChain Tool Calling"
      ],
      "metadata": {
        "id": "JwIXLU7MXOVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/nvidia/NVIDIA-Nemotron-3-Nano-30B-A3B-BF16/resolve/main/nano_v3_reasoning_parser.py"
      ],
      "metadata": {
        "id": "_bP8qtKC10zN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import sys\n",
        "import requests\n",
        "import os\n",
        "\n",
        "# 1. Kill the previous server to free up Port 8000\n",
        "print(\"Stopping old vLLM processes...\")\n",
        "!pkill -f vllm\n",
        "time.sleep(5) # Give it a moment to release the port\n",
        "\n",
        "# 2. Define the command\n",
        "command = [\n",
        "    sys.executable, \"-m\", \"vllm.entrypoints.openai.api_server\",\n",
        "    \"--model\", VLLM_MODEL,\n",
        "    \"--dtype\", \"auto\",\n",
        "    \"--trust-remote-code\",\n",
        "    \"--port\", \"8000\",\n",
        "    \"--gpu-memory-utilization\", \"0.8\",\n",
        "    \"--max-model-len\", str(MAX_MODEL_LEN),\n",
        "    \"--enable-auto-tool-choice\",\n",
        "    \"--tool-call-parser\", \"qwen3_coder\",\n",
        "    \"--reasoning-parser-plugin\", \"nano_v3_reasoning_parser.py\",\n",
        "    \"--reasoning-parser\",\"nano_v3\",\n",
        "]\n",
        "\n",
        "# 3. Start the server in the background\n",
        "with open(\"vllm_logs.txt\", \"w\") as f:\n",
        "    process = subprocess.Popen(command, stdout=f, stderr=f)\n",
        "\n",
        "print(\"Starting vLLM server...\")\n",
        "\n",
        "# 4. Wait for the server to become ready\n",
        "for i in range(360): # Wait up to 6 minutes\n",
        "    try:\n",
        "        response = requests.get(\"http://localhost:8000/health\")\n",
        "        if response.status_code == 200:\n",
        "            print(\"\\nvLLM Server is ready!\")\n",
        "            break\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        pass\n",
        "    print(\".\", end=\"\", flush=True)\n",
        "    time.sleep(1)\n",
        "else:\n",
        "    print(\"\\nTimeout: Server did not start within 6 minutes.\")\n",
        "    print(\"Check content of 'vllm_logs.txt' for errors.\")"
      ],
      "metadata": {
        "id": "zUEdR4WbVA_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "inference_server_url = \"http://localhost:8000/v1\"\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model=VLLM_MODEL,\n",
        "    openai_api_key=\"EMPTY\",\n",
        "    openai_api_base=inference_server_url,\n",
        "    max_tokens=MAX_NEW_TOKENS,\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "ol9yS3kg8nX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "-8y9xsCYHS88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embeddings = HuggingFaceEmbedding(model_name=EMBED_MODEL)"
      ],
      "metadata": {
        "id": "l7ody05BndBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDDu0mDpC6st"
      },
      "outputs": [],
      "source": [
        "SOURCE = userdata.get('DOC_PATH')\n",
        "\n",
        "print(f\"Source document: {SOURCE}\")\n",
        "\n",
        "embed_dim = len(embeddings.get_text_embedding(\"hi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrau49h_p-xI"
      },
      "outputs": [],
      "source": [
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-cEQsFOp-xI"
      },
      "outputs": [],
      "source": [
        "def save_faiss_index(index, path):\n",
        "    \"\"\"Helper function to save FAISS index\"\"\"\n",
        "    faiss.write_index(index, path)\n",
        "    print(f\"FAISS index saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CATUuymYp-xI"
      },
      "outputs": [],
      "source": [
        "def check_faiss_index_status(index):\n",
        "    \"\"\"Check the status of the FAISS index\"\"\"\n",
        "    print(f\"FAISS index contains {index.ntotal} vectors\")\n",
        "    print(f\"Index dimension: {index.d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZP50PWM5qlA"
      },
      "source": [
        "# Single file extraction for Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibt1ARcx5qlA"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.readers.docling import DoclingReader\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "\n",
        "reader = DoclingReader()\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=reader.load_data(SOURCE),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7wXp6ZmFcSe"
      },
      "outputs": [],
      "source": [
        "QUERY = \"Che risultati ha dato l'analisi X fatta nel progetto Y\"\n",
        "\n",
        "result = index.as_query_engine(llm=llm, similarity_top_k=5).query(QUERY)\n",
        "\n",
        "print(\"Query executed successfully!\\n\\n\")\n",
        "\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlrK7Rx5qlB"
      },
      "source": [
        "# Directory Reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVjBYSHZ5qlB"
      },
      "outputs": [],
      "source": [
        "DIR_PATH = \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17GKaKIc5qlB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.readers.docling import DoclingReader\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index reset successfully!\")\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "reader = DoclingReader(export_type=DoclingReader.ExportType.MARKDOWN)\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "dir_reader = SimpleDirectoryReader(\n",
        "    input_dir=DIR_PATH,\n",
        "    file_extractor={\".pdf\": reader,\n",
        "                    \".docx\": reader,\n",
        "                    \".pptx\": reader,\n",
        "                    \".xlsx\": reader,\n",
        "                    },\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=dir_reader.load_data(),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "print(\"Index created successfully!\")\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Test on Directory"
      ],
      "metadata": {
        "id": "ybVECbXPFXjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLQZjkp1p-xJ"
      },
      "outputs": [],
      "source": [
        "QUERY = \"che colla Ã¨ stata usata nel progetto marelli?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdHiQR40p-xJ"
      },
      "outputs": [],
      "source": [
        "result = index.as_query_engine(llm=llm, similarity_top_k=5).query(QUERY)\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.as_retriever(similarity_top_k=5)\n",
        "nodes = retriever.retrieve(QUERY)\n",
        "context_text = \"\\n\\n\".join([n.text for n in nodes])\n",
        "source_files = {n.metadata.get(\"file_name\") for n in nodes if \"file_name\" in n.metadata}\n",
        "\n",
        "print(context_text + \"\\n\\nSources:\\n- \" + \"\\n- \".join(sorted(source_files)))"
      ],
      "metadata": {
        "id": "bwGKIlbt0KKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Prompts"
      ],
      "metadata": {
        "id": "j_vBlfEYFe4l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt for create_agent"
      ],
      "metadata": {
        "id": "d9BKhYjIoFFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a helpful, polite, and conversational AI assistant that answers user questions **only** using information retrieved through the provided tools.\n",
        "\n",
        "## âœ… Rules (Follow Strictly)\n",
        "\n",
        "1. **Always** call a tool before producing a final answer.\n",
        "2. Base your **final answer only** on information retrieved from the tool.\n",
        "3. If the documents do **not** contain the answer, say: \"The requested information is not available in the searched documents.\"\n",
        "4. Respond in the **same language** as the user's question.\n",
        "5. You **must** include the **Sources** section verbatim at the end.\n",
        "6. Never reveal chain-of-thought in the final answer.\n",
        "7. **Always** include the sources you get from the search tool in the answer.\n",
        "8. The tool might provide various informations, feel free to use just the relevant ones.\n",
        "\n",
        "## ðŸ”§ Tools Available\n",
        "\n",
        "You have access to the following tools:\n",
        "\n",
        "- **Document Search**: Performs search on documents based on the user query.\n",
        "Some documents are in English and some are in Italian, it might be useful running the query in both languages to maximize information.\n",
        "You can call it multiple times if you think it could be useful to get more context.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5ePz7akKoAdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Set-up"
      ],
      "metadata": {
        "id": "sbSCMn4F09C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retriever"
      ],
      "metadata": {
        "id": "VS6vpzdlcUFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = index.as_retriever(similarity_top_k=10)"
      ],
      "metadata": {
        "id": "SeEb0SwFbxOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool Definition"
      ],
      "metadata": {
        "id": "OHRg_f3UjlDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.tools import tool\n",
        "\n",
        "# #@tool\n",
        "# def query_and_format_results(query):\n",
        "#     \"\"\" RAG tool for querying documents \"\"\"\n",
        "#     print(\"Invoking RAG tool\")\n",
        "\n",
        "#     nodes = retriever.retrieve(query)\n",
        "\n",
        "#     context_text = \"\\n\\n\".join([n.text for n in nodes])\n",
        "#     source_files = {n.metadata.get(\"file_name\") for n in nodes if \"file_name\" in n.metadata}\n",
        "\n",
        "#     if source_files:\n",
        "#         return context_text + \"\\n\\nSources:\\n- \" + \"\\n- \".join(sorted(source_files))\n",
        "#     return context_text\n"
      ],
      "metadata": {
        "id": "YtYjna0zjhUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool(\n",
        "    \"document_search\",\n",
        "    parse_docstring=True,\n",
        "    description=(\n",
        "        \"Perform search on documents based on the user query\"\n",
        "        \"Use this whenever the user askes questions about documents\"\n",
        "    ),\n",
        ")\n",
        "def query_and_format_results(query):\n",
        "    \"\"\" RAG tool for querying documents \"\"\"\n",
        "    print(\"Invoking RAG tool\")\n",
        "\n",
        "    nodes = retriever.retrieve(query)\n",
        "\n",
        "    context_text = \"\\n\\n\".join([n.text for n in nodes])\n",
        "    source_files = {n.metadata.get(\"file_name\") for n in nodes if \"file_name\" in n.metadata}\n",
        "\n",
        "    if source_files:\n",
        "        return context_text + \"\\n\\nSources:\\n- \" + \"\\n- \".join(sorted(source_files))\n",
        "    return context_text"
      ],
      "metadata": {
        "id": "BlSkjG304JE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using create_agent\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8SV5eTxXbhiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import tool\n",
        "from langchain.agents import create_agent\n",
        "from dataclasses import dataclass\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class RuntimeContext:\n",
        "    db: SQLDatabase\n",
        "\n",
        "agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[query_and_format_results],\n",
        "    system_prompt=template,\n",
        "    context_schema=RuntimeContext,\n",
        "    checkpointer=InMemorySaver(),\n",
        ")"
      ],
      "metadata": {
        "id": "hQSiauPl14XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface"
      ],
      "metadata": {
        "id": "9LafxaRx1AN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def chatbot_response(message, history):\n",
        "  \"\"\"\n",
        "  Receives latest user message, returns agent's response as string.\n",
        "  \"\"\"\n",
        "  result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": message}]},\n",
        "                        {\"configurable\": {\"thread_id\": \"1\"}},\n",
        "                        )\n",
        "\n",
        "  # return result.get(\"output\", result.get(\"output_text\", str(result)))\n",
        "  return (result[\"messages\"][-1].content)\n",
        "\n",
        "\n",
        "interface = gr.ChatInterface(\n",
        "    fn=chatbot_response,\n",
        "    title=\"R&D Projects Chatbot\",\n",
        "    description=\"Ask questions about the R&D projects documents.\",\n",
        "    theme=\"ocean\" # soft, glass, default\n",
        ")\n",
        "\n",
        "interface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "gdrLGpSyqGh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "1uuRHEVXp-xH"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}