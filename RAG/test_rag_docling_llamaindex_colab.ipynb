{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msquareddd/ai-engineering-notebooks/blob/main/RAG/test_rag_docling_llamaindex_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQmkP9Hz5qk9"
      },
      "source": [
        "# RAG with Docling/LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts llama-index-llms-vllm llama-index-core llama-index-readers-docling llama-index-node-parser-docling llama-index-embeddings-huggingface llama-index-readers-file python-dotenv transformers bitsandbytes accelerate faiss-cpu llama-index-vector-stores-faiss vllm llama-index-embeddings-vllm huggingface_hub"
      ],
      "metadata": {
        "id": "DPSW-_kFqKbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnu3rWcw5qlA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "from dotenv import load_dotenv\n",
        "import faiss\n",
        "\n",
        "\n",
        "filterwarnings(action=\"ignore\", category=UserWarning, module=\"pydantic\")\n",
        "filterwarnings(action=\"ignore\", category=FutureWarning, module=\"easyocr\")\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"HF_HOME\"] = \"model/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Set up LangSmith (optional)\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"RAG LlamaIndex\"\n",
        "\n",
        "# HuggingFace login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "9kMf3D7eeI_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJGJ8tS5qlA"
      },
      "source": [
        "We can now define the main parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIfeJNaC3Qi3"
      },
      "outputs": [],
      "source": [
        "MODEL = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"#\"mistralai/Magistral-Small-2509\"#\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\" # \"nemotron-mini:4b\" \"\" \"\"\n",
        "VLLM_MODEL = \"Qwen/Qwen2.5-14B-Instruct\"\n",
        "EMBED_MODEL = \"google/embeddinggemma-300m\" #\"BAAI/bge-small-en-v1.5\"\n",
        "DB_PATH = \"./data/faiss_index.bin\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uuRHEVXp-xH"
      },
      "source": [
        "# Using Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEb_2Drp-xH"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "embeddings = HuggingFaceEmbedding(model_name=EMBED_MODEL)\n",
        "\n",
        "quant_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=MODEL,\n",
        "    tokenizer_name=MODEL,\n",
        "    context_window=8000, # Increased context window size\n",
        "    max_new_tokens=1024,\n",
        "    model_kwargs={\"quantization_config\": quant_config_4bit, \"dtype\": torch.bfloat16},\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True,\"top_p\": 0.95},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using vLLM"
      ],
      "metadata": {
        "id": "qQDbGK-k3q3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.vllm import Vllm\n",
        "\n",
        "# os.environ[\"HUGGINGFACE_API_KEY\"] = \"hf_############################\"\n",
        "\n",
        "llm = Vllm(\n",
        "    model=VLLM_MODEL,\n",
        "    dtype=\"float16\",\n",
        "    tensor_parallel_size=1,\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=512,\n",
        "    vllm_kwargs={\n",
        "        \"swap_space\": 1,\n",
        "        \"gpu_memory_utilization\": 0.6,\n",
        "        \"max_model_len\": 4096,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "9GUvEBv_3qE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.vllm import VllmEmbedding\n",
        "\n",
        "embeddings = VllmEmbedding(\n",
        "        model_name=EMBED_MODEL,\n",
        "        dtype=\"float16\",\n",
        "        tensor_parallel_size=1,\n",
        "        # You can pass additional vllm-specific arguments here\n",
        "        vllm_kwargs={\n",
        "            \"swap_space\": 1,\n",
        "            \"gpu_memory_utilization\": 0.6,\n",
        "        },\n",
        "    )"
      ],
      "metadata": {
        "id": "F9K_1j2TS_WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDDu0mDpC6st"
      },
      "outputs": [],
      "source": [
        "SOURCE = \"/content/data/80019_Marelli_MotorSport_Report.docx\"\n",
        "\n",
        "embed_dim = len(embeddings.get_text_embedding(\"hi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrau49h_p-xI"
      },
      "outputs": [],
      "source": [
        "# if os.path.exists(DB_PATH):\n",
        "#     # Load existing index\n",
        "#     faiss_index = faiss.read_index(DB_PATH)\n",
        "#     print(\"Index loaded successfully!\")\n",
        "# else:\n",
        "#     # Create new index\n",
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-cEQsFOp-xI"
      },
      "outputs": [],
      "source": [
        "def save_faiss_index(index, path):\n",
        "    \"\"\"Helper function to save FAISS index\"\"\"\n",
        "    faiss.write_index(index, path)\n",
        "    print(f\"FAISS index saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CATUuymYp-xI"
      },
      "outputs": [],
      "source": [
        "def check_faiss_index_status(index):\n",
        "    \"\"\"Check the status of the FAISS index\"\"\"\n",
        "    print(f\"FAISS index contains {index.ntotal} vectors\")\n",
        "    print(f\"Index dimension: {index.d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZP50PWM5qlA"
      },
      "source": [
        "# Using Markdown export"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibt1ARcx5qlA"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.readers.docling import DoclingReader\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "\n",
        "reader = DoclingReader()\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=reader.load_data(SOURCE),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7wXp6ZmFcSe"
      },
      "outputs": [],
      "source": [
        "QUERY = \"Che risultati ha dato l'analisi metallografica fatta nel progetto Marelli?\"\n",
        "\n",
        "result = index.as_query_engine(llm=llm, similarity_top_k=5).query(QUERY)\n",
        "\n",
        "print(\"Query executed successfully!\\n\\n\")\n",
        "\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StVyH7mQ5qlB"
      },
      "source": [
        "# Using Docling format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86gfITxS5qlB"
      },
      "source": [
        "To leverage Docling's rich native format, we:\n",
        "- create a `DoclingReader` with JSON export type, and\n",
        "- employ a `DoclingNodeParser` in order to appropriately parse that Docling format.\n",
        "\n",
        "Notice how the sources now also contain document-level grounding (e.g. page number or bounding box information):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jv0EfiAX5qlB"
      },
      "outputs": [],
      "source": [
        "from llama_index.node_parser.docling import DoclingNodeParser\n",
        "\n",
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index reset successfully!\")\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "reader = DoclingReader(export_type=DoclingReader.ExportType.JSON)\n",
        "node_parser = DoclingNodeParser()\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=reader.load_data(SOURCE),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "print(\"Index created successfully!\")\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsTs334Gp-xI"
      },
      "outputs": [],
      "source": [
        "QUERY = \"Che risultati ha dato l'analisi metallografica fatta nel progetto Marelli?\"\n",
        "\n",
        "result = index.as_query_engine(llm=GEN_MODEL, similarity_top_k=5).query(QUERY)\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlrK7Rx5qlB"
      },
      "source": [
        "# With Simple Directory Reader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh99apWQ5qlB"
      },
      "source": [
        "To demonstrate this usage pattern, we first set up a test document directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVjBYSHZ5qlB"
      },
      "outputs": [],
      "source": [
        "DIR_PATH = \"/content/data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruTJKhXo5qlB"
      },
      "source": [
        "Using the `reader` and `node_parser` definitions from any of the above variants, usage with `SimpleDirectoryReader` then looks as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17GKaKIc5qlB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = DoclingReader(export_type=DoclingReader.ExportType.MARKDOWN)\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "dir_reader = SimpleDirectoryReader(\n",
        "    input_dir=DIR_PATH,\n",
        "    file_extractor={\".pdf\": reader,\n",
        "                    \".docx\": reader,\n",
        "                    \".pptx\": reader,\n",
        "                    \".xlsx\": reader,\n",
        "                    },\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=dir_reader.load_data(),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "print(\"Index created successfully!\")\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLQZjkp1p-xJ"
      },
      "outputs": [],
      "source": [
        "QUERY = \"Corrada ha collaborato con UNIVAQ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdHiQR40p-xJ"
      },
      "outputs": [],
      "source": [
        "result = index.as_query_engine(llm=GEN_MODEL, similarity_top_k=5).query(QUERY)\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "1uuRHEVXp-xH"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}