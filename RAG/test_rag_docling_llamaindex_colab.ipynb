{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msquareddd/ai-engineering-notebooks/blob/main/RAG/test_rag_docling_llamaindex_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQmkP9Hz5qk9"
      },
      "source": [
        "# RAG with Docling/LlamaIndex/LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "eT_vFnH0GZ9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts llama-index-llms-vllm llama-index-core llama-index-readers-docling llama-index-node-parser-docling llama-index-embeddings-huggingface llama-index-readers-file python-dotenv transformers bitsandbytes accelerate faiss-cpu llama-index-vector-stores-faiss vllm llama-index-embeddings-vllm huggingface_hub"
      ],
      "metadata": {
        "id": "DPSW-_kFqKbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain gradio langchain-community llama-index-llms-langchain"
      ],
      "metadata": {
        "id": "2FY4s0Hu0bwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnu3rWcw5qlA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "from dotenv import load_dotenv\n",
        "import faiss\n",
        "\n",
        "filterwarnings(action=\"ignore\", category=UserWarning, module=\"pydantic\")\n",
        "filterwarnings(action=\"ignore\", category=FutureWarning, module=\"easyocr\")\n",
        "# # https://github.com/huggingface/transformers/issues/5486:\n",
        "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "# os.environ[\"HF_HOME\"] = \"model/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Login"
      ],
      "metadata": {
        "id": "iq7fA5t1HGoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Set up LangSmith (optional)\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"Test RAG Docling/LlamaIndex\"\n",
        "\n",
        "# HuggingFace login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "9kMf3D7eeI_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Folder Creation"
      ],
      "metadata": {
        "id": "rm89_v9hhCLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content/data\"\n",
        "\n",
        "if not os.path.exists(base_path):\n",
        "    os.makedirs(base_path)\n",
        "    print(f\"Directory '{base_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{base_path}' already exists.\")"
      ],
      "metadata": {
        "id": "cTlaJMxhhBKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJGJ8tS5qlA"
      },
      "source": [
        "# Main Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIfeJNaC3Qi3"
      },
      "outputs": [],
      "source": [
        "MODEL = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"#\"mistralai/Magistral-Small-2509\"#\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\" # \"nemotron-mini:4b\" \"\" \"\"\n",
        "VLLM_MODEL = \"Qwen/Qwen2.5-14B-Instruct-AWQ\" # \"Qwen/Qwen2.5-14B-Thinking-AWQ\" \"Qwen/Qwen2.5-14B-Instruct-AWQ\" #\"Qwen/Qwen2.5-14B-Instruct\"\n",
        "EMBED_MODEL = \"google/embeddinggemma-300m\" #\"BAAI/bge-small-en-v1.5\"\n",
        "DB_PATH = \"./data/faiss_index.bin\"\n",
        "MAX_NEW_TOKENS = 2048\n",
        "QUANT = \"awq_marlin\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uuRHEVXp-xH"
      },
      "source": [
        "# Using Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEb_2Drp-xH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "quant_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=MODEL,\n",
        "    tokenizer_name=MODEL,\n",
        "    context_window=8000, # Increased context window size\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    model_kwargs={\"quantization_config\": quant_config_4bit, \"dtype\": torch.bfloat16},\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True,\"top_p\": 0.95},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using vLLM"
      ],
      "metadata": {
        "id": "qQDbGK-k3q3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import VLLM\n",
        "\n",
        "llm = VLLM(model=VLLM_MODEL,\n",
        "           trust_remote_code=True,  # mandatory for hf models\n",
        "           max_new_tokens=MAX_NEW_TOKENS,\n",
        "           top_k=10,\n",
        "           top_p=0.95,\n",
        "           temperature=0.8,\n",
        "           tensor_parallel_size=1,\n",
        "           vllm_kwargs={\n",
        "            \"quantization\": QUANT,\n",
        "            \"swap_space\": 1,\n",
        "            \"gpu_memory_utilization\": 0.8,\n",
        "            \"max_model_len\": 4096,\n",
        "           }\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yt7f2k0Khe_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "-8y9xsCYHS88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embeddings = HuggingFaceEmbedding(model_name=EMBED_MODEL)"
      ],
      "metadata": {
        "id": "l7ody05BndBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDDu0mDpC6st"
      },
      "outputs": [],
      "source": [
        "SOURCE = userdata.get('DOC_PATH')\n",
        "\n",
        "print(f\"Source document: {SOURCE}\")\n",
        "\n",
        "embed_dim = len(embeddings.get_text_embedding(\"hi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrau49h_p-xI"
      },
      "outputs": [],
      "source": [
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-cEQsFOp-xI"
      },
      "outputs": [],
      "source": [
        "def save_faiss_index(index, path):\n",
        "    \"\"\"Helper function to save FAISS index\"\"\"\n",
        "    faiss.write_index(index, path)\n",
        "    print(f\"FAISS index saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CATUuymYp-xI"
      },
      "outputs": [],
      "source": [
        "def check_faiss_index_status(index):\n",
        "    \"\"\"Check the status of the FAISS index\"\"\"\n",
        "    print(f\"FAISS index contains {index.ntotal} vectors\")\n",
        "    print(f\"Index dimension: {index.d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZP50PWM5qlA"
      },
      "source": [
        "# Single file extraction for Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibt1ARcx5qlA"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.readers.docling import DoclingReader\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "\n",
        "reader = DoclingReader()\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=reader.load_data(SOURCE),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7wXp6ZmFcSe"
      },
      "outputs": [],
      "source": [
        "QUERY = \"Che risultati ha dato l'analisi metallografica fatta nel progetto Marelli?\"\n",
        "\n",
        "result = index.as_query_engine(llm=llm, similarity_top_k=5).query(QUERY)\n",
        "\n",
        "print(\"Query executed successfully!\\n\\n\")\n",
        "\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlrK7Rx5qlB"
      },
      "source": [
        "# Directory Reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVjBYSHZ5qlB"
      },
      "outputs": [],
      "source": [
        "DIR_PATH = \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17GKaKIc5qlB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.readers.docling import DoclingReader\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index reset successfully!\")\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "reader = DoclingReader(export_type=DoclingReader.ExportType.MARKDOWN)\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "dir_reader = SimpleDirectoryReader(\n",
        "    input_dir=DIR_PATH,\n",
        "    file_extractor={\".pdf\": reader,\n",
        "                    \".docx\": reader,\n",
        "                    \".pptx\": reader,\n",
        "                    \".xlsx\": reader,\n",
        "                    },\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=dir_reader.load_data(),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "print(\"Index created successfully!\")\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query Test on Directory"
      ],
      "metadata": {
        "id": "ybVECbXPFXjF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLQZjkp1p-xJ"
      },
      "outputs": [],
      "source": [
        "QUERY = \"Can you summarize the test results in the ZF basekit project?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdHiQR40p-xJ"
      },
      "outputs": [],
      "source": [
        "result = index.as_query_engine(llm=llm, similarity_top_k=5).query(QUERY)\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# System Prompts"
      ],
      "metadata": {
        "id": "j_vBlfEYFe4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"You are a helpful and friendly AI assistant.\n",
        "\n",
        "Follow these rules strictly:\n",
        "1.  Be polite and conversational in your responses.\n",
        "2.  Before answering, you must use the 'Document Search' tool to find relevant information.\n",
        "3.  Base your answers *only* on the information retrieved from the tool. Do not use any of your internal knowledge.\n",
        "4.  If the documents do not contain the answer, state that the information is not available.\n",
        "5.  **Crucially, the 'Document Search' tool will provide a 'Sources' section in its output. You MUST include this 'Sources' section, verbatim and unaltered, at the very end of your final response.**\n",
        "6.  Answer always in the same language of the question.\n",
        "\n",
        "TOOLS:\n",
        "------\n",
        "You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format for reasoning:\n",
        "\n",
        "Question: {input}\n",
        "Thought: you think about what to do\n",
        "Action: one of [{tool_names}]\n",
        "Action Input: the input for the action\n",
        "Observation: result of the action\n",
        "... (you can repeat Thought/Action/Action Input/Observation)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the answer to the original question\n",
        "\n",
        "Begin!\n",
        "\n",
        "{agent_scratchpad}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "903307lEFVSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "# üß† System Prompt (Optimized for LangChain Agent)\n",
        "\n",
        "You are a helpful, polite, and conversational AI assistant that answers user questions **only** using information retrieved through the `Document Search` tool.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Rules (Follow Strictly)\n",
        "\n",
        "1. **Always** call the `Document Search` tool before producing a final answer.\n",
        "   You may call it **multiple times** with different search queries.\n",
        "2. Base your **final answer only** on information retrieved from the tool.\n",
        "   Do **not** use internal knowledge or fabricate information.\n",
        "3. If the documents do **not** contain the answer, say:\n",
        "   **\"The requested information is not available in the searched documents.\"**\n",
        "4. Respond in the **same language** as the user's question.\n",
        "5. The `Document Search` tool returns a **Sources** section.\n",
        "   You **must** include that **Sources** section **verbatim and unaltered** at the **end** of your final answer.\n",
        "6. Never reveal chain-of-thought.\n",
        "   Only output structured action logs using the format below.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Tools Available\n",
        "\n",
        "You have access to the following tools:\n",
        "\n",
        "```\n",
        "{tools}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß© Reasoning & Action Format\n",
        "\n",
        "Use this exact structure to perform tool calls and produce your final answer:\n",
        "\n",
        "```\n",
        "Question: {input}\n",
        "\n",
        "Thought: you think about what to do\n",
        "Action: one of [{tool_names}]\n",
        "Action Input: {{search query}}\n",
        "Observation: {{result}}\n",
        "\n",
        "# You may repeat Thought ‚Üí Action ‚Üí Action Input ‚Üí Observation as needed.\n",
        "\n",
        "Thought: I now know the final answer\n",
        "Final Answer: {{answer strictly based on observations}}\n",
        "\n",
        "Sources:\n",
        "{{verbatim Sources section from the final tool output}}\n",
        "```\n",
        "\n",
        "- You are allowed and encouraged to run **multiple search queries**, trying different variations, synonyms, keyword versions, broader/narrower phrasing, etc.\n",
        "- Do **not** modify the `Sources` text.\n",
        "\n",
        "---\n",
        "\n",
        "## üîç Query Variation Guidance\n",
        "\n",
        "When performing searches, you may generate variations of the user query such as:\n",
        "\n",
        "- The exact user question\n",
        "- Synonym-expanded versions\n",
        "- Rephrased or keyword-based versions\n",
        "- Broader or narrower versions\n",
        "- Versions with or without filters/qualifiers\n",
        "\n",
        "Stop once you have found enough information to answer.\n",
        "\n",
        "---\n",
        "\n",
        "## üö´ Failure Handling\n",
        "\n",
        "If the tool returns an error or no data, say:\n",
        "\n",
        "**\"Document Search failed: {{error message}}\"**\n",
        "or\n",
        "**\"The requested information is not available in the searched documents.\"**\n",
        "\n",
        "Do **not** answer using internal knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ñ∂Ô∏è Begin\n",
        "\n",
        "{agent_scratchpad}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "caay5VdPDfBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Set-up"
      ],
      "metadata": {
        "id": "sbSCMn4F09C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "from langchain.tools import Tool\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "\n",
        "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
        "\n",
        "def query_and_format_results(query):\n",
        "    result = query_engine.query(query)\n",
        "    answer = result.response.strip()\n",
        "\n",
        "    source_files = {n.metadata.get(\"file_name\") for n in result.source_nodes if \"file_name\" in n.metadata}\n",
        "    if source_files:\n",
        "        return answer + \"\\n\\nSources:\\n- \" + \"\\n- \".join(sorted(source_files))\n",
        "    return answer\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Document Search\",\n",
        "        func=query_and_format_results,\n",
        "        description=\"Use this to answer questions using the uploaded documents.\",\n",
        "    )\n",
        "]\n",
        "\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"input\", \"tools\", \"tool_names\", \"agent_scratchpad\"],\n",
        "    template=template,\n",
        "    tools=tools,\n",
        ")\n",
        "\n",
        "react_agent = create_react_agent(\n",
        "    llm=llm,\n",
        "    tools=tools,\n",
        "    prompt=prompt,\n",
        ")\n",
        "\n",
        "agent_chain = AgentExecutor(\n",
        "    agent=react_agent,\n",
        "    tools=tools,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    handle_parsing_errors=True,\n",
        ")"
      ],
      "metadata": {
        "id": "hQSiauPl14XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface"
      ],
      "metadata": {
        "id": "9LafxaRx1AN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def chatbot_response(message, history):\n",
        "  \"\"\"\n",
        "  Receives latest user message, returns agent's response as string.\n",
        "  \"\"\"\n",
        "  result = agent_chain.invoke({\"input\": message})\n",
        "  return result.get(\"output\", result.get(\"output_text\", str(result)))\n",
        "\n",
        "\n",
        "interface = gr.ChatInterface(\n",
        "    fn=chatbot_response,\n",
        "    title=\"R&D Projects Chatbot\",\n",
        "    description=\"Ask questions about the R&D projects documents.\",\n",
        "    theme=\"ocean\" # soft, glass, default\n",
        ")\n",
        "\n",
        "interface.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "YqdtvbEP1Bkl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "1uuRHEVXp-xH"
      ],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}