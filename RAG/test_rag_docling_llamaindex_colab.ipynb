{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msquareddd/ai-engineering-notebooks/blob/main/RAG/test_rag_docling_llamaindex_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQmkP9Hz5qk9"
      },
      "source": [
        "# RAG with Docling/LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q --progress-bar off --no-warn-conflicts llama-index-llms-vllm llama-index-core llama-index-readers-docling llama-index-node-parser-docling llama-index-embeddings-huggingface llama-index-readers-file python-dotenv transformers bitsandbytes accelerate faiss-cpu llama-index-vector-stores-faiss vllm llama-index-embeddings-vllm huggingface_hub"
      ],
      "metadata": {
        "id": "DPSW-_kFqKbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q langchain gradio"
      ],
      "metadata": {
        "id": "2FY4s0Hu0bwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnu3rWcw5qlA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "from dotenv import load_dotenv\n",
        "import faiss\n",
        "\n",
        "\n",
        "filterwarnings(action=\"ignore\", category=UserWarning, module=\"pydantic\")\n",
        "filterwarnings(action=\"ignore\", category=FutureWarning, module=\"easyocr\")\n",
        "# https://github.com/huggingface/transformers/issues/5486:\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ[\"HF_HOME\"] = \"model/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Set up LangSmith (optional)\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ[\"LANGSMITH_PROJECT\"] = \"Test RAG Docling/LlamaIndex\"\n",
        "\n",
        "# HuggingFace login\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "9kMf3D7eeI_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNJGJ8tS5qlA"
      },
      "source": [
        "We can now define the main parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIfeJNaC3Qi3"
      },
      "outputs": [],
      "source": [
        "MODEL = \"Qwen/Qwen3-30B-A3B-Instruct-2507\"#\"mistralai/Magistral-Small-2509\"#\"nvidia/NVIDIA-Nemotron-Nano-9B-v2\" # \"nemotron-mini:4b\" \"\" \"\"\n",
        "VLLM_MODEL = \"Qwen/Qwen2.5-14B-Instruct\"\n",
        "EMBED_MODEL = \"google/embeddinggemma-300m\" #\"BAAI/bge-small-en-v1.5\"\n",
        "DB_PATH = \"./data/faiss_index.bin\"\n",
        "MAX_NEW_TOKENS = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uuRHEVXp-xH"
      },
      "source": [
        "# Using Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIEb_2Drp-xH"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "quant_config_4bit = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=MODEL,\n",
        "    tokenizer_name=MODEL,\n",
        "    context_window=8000, # Increased context window size\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    model_kwargs={\"quantization_config\": quant_config_4bit, \"dtype\": torch.bfloat16},\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True,\"top_p\": 0.95},\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using vLLM"
      ],
      "metadata": {
        "id": "qQDbGK-k3q3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.vllm import Vllm\n",
        "\n",
        "llm = Vllm(\n",
        "    model=VLLM_MODEL,\n",
        "    dtype=\"float16\",\n",
        "    tensor_parallel_size=1,\n",
        "    temperature=0.9,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    vllm_kwargs={\n",
        "        \"swap_space\": 1,\n",
        "        \"gpu_memory_utilization\": 0.6,\n",
        "        \"max_model_len\": 4096,\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "9GUvEBv_3qE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "\n",
        "embeddings = HuggingFaceEmbedding(model_name=EMBED_MODEL)"
      ],
      "metadata": {
        "id": "l7ody05BndBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDDu0mDpC6st"
      },
      "outputs": [],
      "source": [
        "SOURCE = userdata.get('DOC_PATH')\n",
        "\n",
        "print(f\"Source document: {SOURCE}\")\n",
        "\n",
        "embed_dim = len(embeddings.get_text_embedding(\"hi\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rrau49h_p-xI"
      },
      "outputs": [],
      "source": [
        "# if os.path.exists(DB_PATH):\n",
        "#     # Load existing index\n",
        "#     faiss_index = faiss.read_index(DB_PATH)\n",
        "#     print(\"Index loaded successfully!\")\n",
        "# else:\n",
        "#     # Create new index\n",
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-cEQsFOp-xI"
      },
      "outputs": [],
      "source": [
        "def save_faiss_index(index, path):\n",
        "    \"\"\"Helper function to save FAISS index\"\"\"\n",
        "    faiss.write_index(index, path)\n",
        "    print(f\"FAISS index saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CATUuymYp-xI"
      },
      "outputs": [],
      "source": [
        "def check_faiss_index_status(index):\n",
        "    \"\"\"Check the status of the FAISS index\"\"\"\n",
        "    print(f\"FAISS index contains {index.ntotal} vectors\")\n",
        "    print(f\"Index dimension: {index.d}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZP50PWM5qlA"
      },
      "source": [
        "# Single file extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibt1ARcx5qlA"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext, VectorStoreIndex\n",
        "from llama_index.core.node_parser import MarkdownNodeParser\n",
        "from llama_index.readers.docling import DoclingReader\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "\n",
        "reader = DoclingReader()\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=reader.load_data(SOURCE),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7wXp6ZmFcSe"
      },
      "outputs": [],
      "source": [
        "QUERY = \"Che risultati ha dato l'analisi metallografica fatta nel progetto Marelli?\"\n",
        "\n",
        "result = index.as_query_engine(llm=llm, similarity_top_k=5).query(QUERY)\n",
        "\n",
        "print(\"Query executed successfully!\\n\\n\")\n",
        "\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJlrK7Rx5qlB"
      },
      "source": [
        "# Directory Reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVjBYSHZ5qlB"
      },
      "outputs": [],
      "source": [
        "DIR_PATH = \"/content/data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17GKaKIc5qlB"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "faiss_index = faiss.IndexFlatL2(embed_dim)\n",
        "print(\"Index reset successfully!\")\n",
        "\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "print(\"Vector store created successfully!\")\n",
        "\n",
        "reader = DoclingReader(export_type=DoclingReader.ExportType.MARKDOWN)\n",
        "node_parser = MarkdownNodeParser()\n",
        "\n",
        "dir_reader = SimpleDirectoryReader(\n",
        "    input_dir=DIR_PATH,\n",
        "    file_extractor={\".pdf\": reader,\n",
        "                    \".docx\": reader,\n",
        "                    \".pptx\": reader,\n",
        "                    \".xlsx\": reader,\n",
        "                    },\n",
        ")\n",
        "\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents=dir_reader.load_data(),\n",
        "    transformations=[node_parser],\n",
        "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
        "    embed_model=embeddings,\n",
        ")\n",
        "\n",
        "print(\"Index created successfully!\")\n",
        "save_faiss_index(faiss_index, DB_PATH)\n",
        "check_faiss_index_status(faiss_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLQZjkp1p-xJ"
      },
      "outputs": [],
      "source": [
        "QUERY = \"In the marelli project, what were the results on the test mad on the rings?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdHiQR40p-xJ"
      },
      "outputs": [],
      "source": [
        "result = index.as_query_engine(llm=llm, similarity_top_k=5).query(QUERY)\n",
        "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n\\nSources:\")\n",
        "display([(n.text, n.metadata) for n in result.source_nodes])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain Set-up"
      ],
      "metadata": {
        "id": "sbSCMn4F09C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this new import\n",
        "from langchain.llms import LlamaIndexLLM\n",
        "from langchain.prompts import SystemMessage\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# 1. Convert the LlamaIndex query engine into a LangChain tool\n",
        "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
        "\n",
        "# 2. Define the generic RAG tool for the LangChain agent\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Document Search\",\n",
        "        func=lambda q: str(query_engine.query(q)),\n",
        "        description=\"Useful for finding and answering questions about the information contained in the uploaded documents.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 3. Set up memory for the chatbot\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# 4. Wrap the LlamaIndex LLM for use in LangChain\n",
        "langchain_llm = LlamaIndexLLM(llm=llm)\n",
        "\n",
        "# 5. DEFINE THE SYSTEM PROMPT\n",
        "system_prompt_content = \"\"\"You are a helpful and friendly AI assistant.\n",
        "\n",
        "Your primary purpose is to answer questions based on the information available in the documents provided through the 'Document Search' tool.\n",
        "\n",
        "Follow these rules strictly:\n",
        "1.  Be polite and conversational in your responses.\n",
        "2.  Before answering, you must use the 'Document Search' tool to find relevant information.\n",
        "3.  Base your answers *only* on the information retrieved from the tool. Do not use any of your internal knowledge.\n",
        "4.  If the documents do not contain the answer to a question, you must clearly state that the information is not available in the provided documents instead of trying to make up an answer.\n",
        "\"\"\"\n",
        "\n",
        "# 6. Set up agent_kwargs with the system message\n",
        "agent_kwargs = {\n",
        "    \"system_message\": SystemMessage(content=system_prompt_content)\n",
        "}\n",
        "\n",
        "# 7. Initialize the LangChain agent with the system prompt\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    langchain_llm,\n",
        "    agent=\"conversational-react-description\",\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    agent_kwargs=agent_kwargs,\n",
        ")\n"
      ],
      "metadata": {
        "id": "tULrgnaM0n-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With Sources"
      ],
      "metadata": {
        "id": "b_xys-Xs15h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this new import\n",
        "from langchain.llms import LlamaIndexLLM\n",
        "from langchain.prompts import SystemMessage\n",
        "\n",
        "from langchain.agents import Tool\n",
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# 1. Convert the LlamaIndex query engine into a LangChain tool\n",
        "query_engine = index.as_query_engine(llm=llm, similarity_top_k=5)\n",
        "\n",
        "# 2. Define the generic RAG tool for the LangChain agent\n",
        "#    This function now formats the sources into the final string.\n",
        "def query_and_format_results(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Queries the LlamaIndex engine and formats the response with sources.\n",
        "    \"\"\"\n",
        "    result = query_engine.query(query)\n",
        "    answer = result.response.strip()\n",
        "\n",
        "    # Extract unique source filenames from metadata\n",
        "    source_files = set()\n",
        "    for node in result.source_nodes:\n",
        "        # The filename is usually stored in the metadata\n",
        "        if 'file_name' in node.metadata:\n",
        "            source_files.add(node.metadata['file_name'])\n",
        "\n",
        "    # Format the final output string\n",
        "    if source_files:\n",
        "        sources_text = \"\\n\\nSources:\\n- \" + \"\\n- \".join(sorted(list(source_files)))\n",
        "        return f\"{answer}{sources_text}\"\n",
        "    else:\n",
        "        return answer\n",
        "\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Document Search\",\n",
        "        func=query_and_format_results,\n",
        "        description=\"Useful for finding and answering questions about the information contained in the uploaded documents. Provides the answer and the source documents.\",\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 3. Set up memory for the chatbot\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "# 4. Wrap the LlamaIndex LLM for use in LangChain\n",
        "langchain_llm = LlamaIndexLLM(llm=llm)\n",
        "\n",
        "# 5. DEFINE THE UPDATED SYSTEM PROMPT\n",
        "system_prompt_content = \"\"\"You are a helpful and friendly AI assistant named 'DocuBot'.\n",
        "\n",
        "Your primary purpose is to answer questions based on the information available in the documents provided through the 'Document Search' tool.\n",
        "\n",
        "Follow these rules strictly:\n",
        "1.  Be polite and conversational in your responses.\n",
        "2.  Before answering, you must use the 'Document Search' tool to find relevant information.\n",
        "3.  Base your answers *only* on the information retrieved from the tool. Do not use any of your internal knowledge.\n",
        "4.  If the documents do not contain the answer, state that the information is not available.\n",
        "5.  **Crucially, the 'Document Search' tool will provide a 'Sources' section in its output. You MUST include this 'Sources' section, verbatim and unaltered, at the very end of your final response.**\n",
        "6.  Answer always in the same language of the question.\n",
        "\"\"\"\n",
        "\n",
        "# 6. Set up agent_kwargs with the system message\n",
        "agent_kwargs = {\n",
        "    \"system_message\": SystemMessage(content=system_prompt_content)\n",
        "}\n",
        "\n",
        "# 7. Initialize the LangChain agent with the system prompt\n",
        "agent_chain = initialize_agent(\n",
        "    tools,\n",
        "    langchain_llm,\n",
        "    agent=\"conversational-react-description\",\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    agent_kwargs=agent_kwargs,\n",
        ")"
      ],
      "metadata": {
        "id": "hQSiauPl14XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio Interface"
      ],
      "metadata": {
        "id": "9LafxaRx1AN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chatbot_response(message, history):\n",
        "    \"\"\"\n",
        "    Function to get the response from the LangChain agent.\n",
        "    \"\"\"\n",
        "    response = agent_chain.run(input=message)\n",
        "    return response\n",
        "\n",
        "interface = gr.ChatInterface(\n",
        "    fn=chatbot_response,\n",
        "    title=\"R&D Projects Chatbot\",\n",
        "    description=\"Ask questions about the R&D projects documents.\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "interface.launch(share=True, debug=True) # Set share=True to get a public link"
      ],
      "metadata": {
        "id": "YqdtvbEP1Bkl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "collapsed_sections": [
        "1uuRHEVXp-xH"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}